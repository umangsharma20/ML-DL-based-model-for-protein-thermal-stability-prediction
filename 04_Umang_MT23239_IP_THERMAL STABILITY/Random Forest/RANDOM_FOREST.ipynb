{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas scikit-learn biopython lazypredict\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRrvHtk2Ffj4",
        "outputId": "8755060c-42e6-4d72-d342-85a2a0192819"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Collecting biopython\n",
            "  Downloading biopython-1.84-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting lazypredict\n",
            "  Downloading lazypredict-0.2.12-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from lazypredict) (8.1.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from lazypredict) (4.66.4)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (from lazypredict) (4.4.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (from lazypredict) (2.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Collecting nvidia-nccl-cu12 (from xgboost->lazypredict)\n",
            "  Downloading nvidia_nccl_cu12-2.22.3-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Downloading biopython-1.84-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lazypredict-0.2.12-py2.py3-none-any.whl (12 kB)\n",
            "Downloading nvidia_nccl_cu12-2.22.3-py3-none-manylinux2014_x86_64.whl (190.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.9/190.9 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nccl-cu12, biopython, lazypredict\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.3.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nccl-cu12 2.22.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed biopython-1.84 lazypredict-0.2.12 nvidia-nccl-cu12-2.22.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from lazypredict.Supervised import LazyRegressor\n",
        "from scipy.stats import pearsonr\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = pd.read_csv('/content/PDB+TM+FASTA (1).csv')\n",
        "\n",
        "# Step 2: Extract features using biopython\n",
        "def extract_biopython_features(sequence):\n",
        "    try:\n",
        "        sequence = sequence.replace('\\r', '').replace('\\n', '')\n",
        "        analysis = ProteinAnalysis(sequence)\n",
        "        features = {\n",
        "            'molecular_weight': analysis.molecular_weight(),\n",
        "            'aromaticity': analysis.aromaticity(),\n",
        "            'instability_index': analysis.instability_index(),\n",
        "            'isoelectric_point': analysis.isoelectric_point(),\n",
        "            'gravy': analysis.gravy(),\n",
        "            'flexibility': sum(analysis.flexibility()) / len(sequence),\n",
        "            'charge_at_pH_7': analysis.charge_at_pH(7.0),\n",
        "        }\n",
        "        return features\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing sequence: {sequence}. Error: {e}\")\n",
        "        return None\n",
        "\n",
        "sequences = data['Sequence'].tolist()\n",
        "biopython_features = [extract_biopython_features(seq) for seq in sequences]\n",
        "biopython_features = [feat for feat in biopython_features if feat is not None]  # Remove None entries\n",
        "biopython_features_df = pd.DataFrame(biopython_features)\n",
        "\n",
        "# Step 3: Ensure the length of the sequences matches the length of the melting_temp\n",
        "if len(biopython_features_df) != len(data['melting_temp']):\n",
        "    raise ValueError(\"Mismatch between number of sequences and melting temperatures\")\n",
        "\n",
        "# Step 4: Combine features with stability values (as continuous values)\n",
        "combined_df = pd.concat([biopython_features_df, data['melting_temp'].iloc[:len(biopython_features_df)]], axis=1)\n",
        "\n",
        "# Step 5: Split data into training and testing sets\n",
        "X = combined_df.drop(columns=['melting_temp']).values\n",
        "y = combined_df['melting_temp'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 7: Apply LazyPredict for model evaluation\n",
        "reg = LazyRegressor(ignore_warnings=True, random_state=42)\n",
        "models, predictions = reg.fit(X_train_scaled, X_test_scaled, y_train, y_test)\n",
        "\n",
        "# Check shapes to debug\n",
        "print(\"Shapes: \", X_train_scaled.shape, X_test_scaled.shape, y_train.shape, y_test.shape)\n",
        "print(\"Predictions shape: \", predictions.shape)\n",
        "\n",
        "# Step 8: Calculate Pearson correlation coefficient\n",
        "pearson_results = {}\n",
        "for model in predictions.columns:\n",
        "    if len(predictions[model]) != len(y_test):\n",
        "        print(f\"Skipping model {model} due to length mismatch.\")\n",
        "        continue\n",
        "    pearson_corr, _ = pearsonr(y_test, predictions[model])\n",
        "    pearson_results[model] = pearson_corr\n",
        "\n",
        "# Step 9: Display Pearson correlation coefficients\n",
        "for model, pearson_corr in pearson_results.items():\n",
        "    print(f\"Pearson correlation for {model}: {pearson_corr:.4f}\")\n",
        "\n",
        "# Step 10: Display model performance with Pearson correlation\n",
        "models['Pearson Correlation'] = models.index.map(pearson_results.get)\n",
        "print(models)\n",
        "\n",
        "# Step 11: Select the best performing model based on Pearson correlation\n",
        "if models['Pearson Correlation'].isnull().all():\n",
        "    print(\"No valid Pearson correlations calculated. Check for issues in model predictions.\")\n",
        "else:\n",
        "    best_model = models.loc[models['Pearson Correlation'].idxmax()]\n",
        "    print(\"\\nBest model based on Pearson correlation coefficient:\")\n",
        "    print(best_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtgHaCRAkZCx",
        "outputId": "52d4344a-f322-41f3-e3d3-7ba78bea7be6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "100%|██████████| 42/42 [00:04<00:00,  9.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000146 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 657\n",
            "[LightGBM] [Info] Number of data points in the train set: 279, number of used features: 7\n",
            "[LightGBM] [Info] Start training from score 64.132101\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Shapes:  (279, 7) (70, 7) (279,) (70,)\n",
            "Predictions shape:  (41, 4)\n",
            "Skipping model Adjusted R-Squared due to length mismatch.\n",
            "Skipping model R-Squared due to length mismatch.\n",
            "Skipping model RMSE due to length mismatch.\n",
            "Skipping model Time Taken due to length mismatch.\n",
            "                               Adjusted R-Squared  R-Squared  RMSE  \\\n",
            "Model                                                                \n",
            "RandomForestRegressor                        0.15       0.23 17.47   \n",
            "SVR                                          0.11       0.20 17.82   \n",
            "ExtraTreesRegressor                          0.09       0.18 18.01   \n",
            "BaggingRegressor                             0.06       0.16 18.31   \n",
            "GradientBoostingRegressor                    0.06       0.15 18.34   \n",
            "AdaBoostRegressor                            0.06       0.15 18.35   \n",
            "NuSVR                                        0.05       0.15 18.38   \n",
            "BayesianRidge                                0.03       0.13 18.59   \n",
            "ElasticNetCV                                 0.03       0.13 18.64   \n",
            "RidgeCV                                      0.02       0.12 18.69   \n",
            "SGDRegressor                                 0.02       0.12 18.73   \n",
            "LarsCV                                       0.01       0.11 18.75   \n",
            "LassoCV                                      0.01       0.11 18.75   \n",
            "LassoLarsCV                                  0.01       0.11 18.75   \n",
            "Lasso                                        0.01       0.11 18.75   \n",
            "LassoLars                                    0.01       0.11 18.75   \n",
            "LassoLarsIC                                  0.01       0.11 18.77   \n",
            "Ridge                                        0.01       0.11 18.80   \n",
            "Lars                                         0.01       0.11 18.81   \n",
            "LinearRegression                             0.01       0.11 18.81   \n",
            "TransformedTargetRegressor                   0.01       0.11 18.81   \n",
            "ElasticNet                                   0.01       0.11 18.81   \n",
            "HuberRegressor                               0.01       0.11 18.83   \n",
            "OrthogonalMatchingPursuitCV                  0.01       0.11 18.83   \n",
            "TweedieRegressor                            -0.00       0.10 18.89   \n",
            "GammaRegressor                              -0.01       0.09 19.00   \n",
            "OrthogonalMatchingPursuit                   -0.01       0.09 19.01   \n",
            "XGBRegressor                                -0.01       0.09 19.03   \n",
            "PoissonRegressor                            -0.03       0.07 19.18   \n",
            "LGBMRegressor                               -0.07       0.04 19.53   \n",
            "HistGradientBoostingRegressor               -0.08       0.03 19.65   \n",
            "KNeighborsRegressor                         -0.10       0.01 19.83   \n",
            "DummyRegressor                              -0.13      -0.02 20.11   \n",
            "PassiveAggressiveRegressor                  -0.16      -0.05 20.38   \n",
            "LinearSVR                                   -0.19      -0.07 20.61   \n",
            "ExtraTreeRegressor                          -0.44      -0.30 22.70   \n",
            "DecisionTreeRegressor                       -0.47      -0.32 22.87   \n",
            "RANSACRegressor                             -1.25      -1.02 28.34   \n",
            "GaussianProcessRegressor                    -2.76      -2.38 36.61   \n",
            "MLPRegressor                                -2.98      -2.58 37.71   \n",
            "KernelRidge                                -12.13     -10.80 68.45   \n",
            "\n",
            "                               Time Taken Pearson Correlation  \n",
            "Model                                                          \n",
            "RandomForestRegressor                0.24                None  \n",
            "SVR                                  0.01                None  \n",
            "ExtraTreesRegressor                  0.28                None  \n",
            "BaggingRegressor                     0.11                None  \n",
            "GradientBoostingRegressor            0.36                None  \n",
            "AdaBoostRegressor                    0.14                None  \n",
            "NuSVR                                0.03                None  \n",
            "BayesianRidge                        0.06                None  \n",
            "ElasticNetCV                         0.23                None  \n",
            "RidgeCV                              0.01                None  \n",
            "SGDRegressor                         0.01                None  \n",
            "LarsCV                               0.05                None  \n",
            "LassoCV                              0.18                None  \n",
            "LassoLarsCV                          0.03                None  \n",
            "Lasso                                0.04                None  \n",
            "LassoLars                            0.03                None  \n",
            "LassoLarsIC                          0.02                None  \n",
            "Ridge                                0.01                None  \n",
            "Lars                                 0.03                None  \n",
            "LinearRegression                     0.01                None  \n",
            "TransformedTargetRegressor           0.01                None  \n",
            "ElasticNet                           0.02                None  \n",
            "HuberRegressor                       0.03                None  \n",
            "OrthogonalMatchingPursuitCV          0.05                None  \n",
            "TweedieRegressor                     0.01                None  \n",
            "GammaRegressor                       0.03                None  \n",
            "OrthogonalMatchingPursuit            0.05                None  \n",
            "XGBRegressor                         0.23                None  \n",
            "PoissonRegressor                     0.02                None  \n",
            "LGBMRegressor                        0.11                None  \n",
            "HistGradientBoostingRegressor        0.28                None  \n",
            "KNeighborsRegressor                  0.01                None  \n",
            "DummyRegressor                       0.02                None  \n",
            "PassiveAggressiveRegressor           0.03                None  \n",
            "LinearSVR                            0.02                None  \n",
            "ExtraTreeRegressor                   0.03                None  \n",
            "DecisionTreeRegressor                0.03                None  \n",
            "RANSACRegressor                      0.09                None  \n",
            "GaussianProcessRegressor             0.57                None  \n",
            "MLPRegressor                         0.97                None  \n",
            "KernelRidge                          0.02                None  \n",
            "No valid Pearson correlations calculated. Check for issues in model predictions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('/content/PDB+TM+FASTA (1).csv')\n",
        "\n",
        "# Extract features using biopython\n",
        "def extract_biopython_features(sequence):\n",
        "    try:\n",
        "        sequence = sequence.replace('\\r', '').replace('\\n', '')\n",
        "        analysis = ProteinAnalysis(sequence)\n",
        "        features = {\n",
        "            'molecular_weight': analysis.molecular_weight(),\n",
        "            'aromaticity': analysis.aromaticity(),\n",
        "            'instability_index': analysis.instability_index(),\n",
        "            'isoelectric_point': analysis.isoelectric_point(),\n",
        "            'gravy': analysis.gravy(),\n",
        "            'flexibility': sum(analysis.flexibility()),\n",
        "            'charge_at_pH_7': analysis.charge_at_pH(7.0),\n",
        "            'amino_acid_comp': analysis.count_amino_acids()\n",
        "        }\n",
        "        return features\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing sequence: {sequence}. Error: {e}\")\n",
        "        return None\n",
        "\n",
        "sequences = data['Sequence'].tolist()\n",
        "biopython_features = [extract_biopython_features(seq) for seq in sequences]\n",
        "biopython_features = [feat for feat in biopython_features if feat is not None]\n",
        "\n",
        "# Convert amino acid composition dicts to separate columns\n",
        "amino_acids = ['A', 'R', 'N', 'D', 'C', 'E', 'Q', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
        "for feature in biopython_features:\n",
        "    for aa in amino_acids:\n",
        "        feature[f'aa_{aa}'] = feature['amino_acid_comp'].get(aa, 0)\n",
        "    del feature['amino_acid_comp']\n",
        "\n",
        "biopython_features_df = pd.DataFrame(biopython_features)\n",
        "\n",
        "# Ensure the length of the sequences matches the length of the melting_temp\n",
        "if len(biopython_features_df) != len(data['melting_temp']):\n",
        "    raise ValueError(\"Mismatch between number of sequences and melting temperatures\")\n",
        "\n",
        "# Combine features with stability values\n",
        "combined_df = pd.concat([biopython_features_df, data['melting_temp'].iloc[:len(biopython_features_df)]], axis=1)\n",
        "\n",
        "# Remove outliers (optional, depends on your dataset)\n",
        "def remove_outliers(df, threshold=3):\n",
        "    z_scores = np.abs((df - df.mean()) / df.std())\n",
        "    return df[(z_scores < threshold).all(axis=1)]\n",
        "\n",
        "combined_df = remove_outliers(combined_df)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X = combined_df.drop(columns=['melting_temp']).values\n",
        "y = combined_df['melting_temp'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Hyperparameter tuning for RandomForestRegressor and GradientBoostingRegressor\n",
        "param_grid_rf = {\n",
        "    'rf__n_estimators': [100, 200, 300],\n",
        "    'rf__max_depth': [None, 10, 20, 30],\n",
        "    'rf__min_samples_split': [2, 5, 10],\n",
        "    'rf__min_samples_leaf': [1, 2, 4],\n",
        "    'rf__bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "param_grid_gb = {\n",
        "    'gb__n_estimators': [100, 200, 300],\n",
        "    'gb__learning_rate': [0.01, 0.1, 0.2],\n",
        "    'gb__max_depth': [3, 5, 7],\n",
        "    'gb__min_samples_split': [2, 5, 10],\n",
        "    'gb__min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "pipeline_rf = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('rf', RandomForestRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "pipeline_gb = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('gb', GradientBoostingRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "grid_search_rf = GridSearchCV(estimator=pipeline_rf, param_grid=param_grid_rf, cv=5, n_jobs=-1, verbose=2)\n",
        "grid_search_gb = GridSearchCV(estimator=pipeline_gb, param_grid=param_grid_gb, cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "# Train the models and find the best parameters\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "grid_search_gb.fit(X_train, y_train)\n",
        "\n",
        "best_rf = grid_search_rf.best_estimator_\n",
        "best_gb = grid_search_gb.best_estimator_\n",
        "\n",
        "# Make predictions with the best models\n",
        "y_train_pred_rf = best_rf.predict(X_train)\n",
        "y_test_pred_rf = best_rf.predict(X_test)\n",
        "y_train_pred_gb = best_gb.predict(X_train)\n",
        "y_test_pred_gb = best_gb.predict(X_test)\n",
        "\n",
        "# Calculate accuracy metrics for both models\n",
        "train_rmse_rf = mean_squared_error(y_train, y_train_pred_rf, squared=False)\n",
        "test_rmse_rf = mean_squared_error(y_test, y_test_pred_rf, squared=False)\n",
        "train_r2_rf = r2_score(y_train, y_train_pred_rf)\n",
        "test_r2_rf = r2_score(y_test, y_test_pred_rf)\n",
        "\n",
        "train_rmse_gb = mean_squared_error(y_train, y_train_pred_gb, squared=False)\n",
        "test_rmse_gb = mean_squared_error(y_test, y_test_pred_gb, squared=False)\n",
        "train_r2_gb = r2_score(y_train, y_train_pred_gb)\n",
        "test_r2_gb = r2_score(y_test, y_test_pred_gb)\n",
        "\n",
        "print(f\"Random Forest Best Parameters: {grid_search_rf.best_params_}\")\n",
        "print(f\"Random Forest Training RMSE: {train_rmse_rf}\")\n",
        "print(f\"Random Forest Testing RMSE: {test_rmse_rf}\")\n",
        "print(f\"Random Forest Training R²: {train_r2_rf}\")\n",
        "print(f\"Random Forest Testing R²: {test_r2_rf}\")\n",
        "\n",
        "print(f\"Gradient Boosting Best Parameters: {grid_search_gb.best_params_}\")\n",
        "print(f\"Gradient Boosting Training RMSE: {train_rmse_gb}\")\n",
        "print(f\"Gradient Boosting Testing RMSE: {test_rmse_gb}\")\n",
        "print(f\"Gradient Boosting Training R²: {train_r2_gb}\")\n",
        "print(f\"Gradient Boosting Testing R²: {test_r2_gb}\")\n",
        "\n",
        "# Cross-validation scores for more reliable performance estimation\n",
        "cv_rf = cross_val_score(best_rf, X, y, cv=5, scoring='r2')\n",
        "cv_gb = cross_val_score(best_gb, X, y, cv=5, scoring='r2')\n",
        "\n",
        "print(f\"Random Forest Cross-Validation R² Scores: {cv_rf}\")\n",
        "print(f\"Gradient Boosting Cross-Validation R² Scores: {cv_gb}\")\n",
        "print(f\"Random Forest Average Cross-Validation R² Score: {np.mean(cv_rf)}\")\n",
        "print(f\"Gradient Boosting Average Cross-Validation R² Score: {np.mean(cv_gb)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RhGN1wvJZFQ",
        "outputId": "3e249d9a-7754-4db2-e770-ce2b54cec572"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n",
            "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
            "Random Forest Best Parameters: {'rf__bootstrap': True, 'rf__max_depth': None, 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 2, 'rf__n_estimators': 100}\n",
            "Random Forest Training RMSE: 5.421423118832182\n",
            "Random Forest Testing RMSE: 11.671268749527682\n",
            "Random Forest Training R²: 0.9214679051614564\n",
            "Random Forest Testing R²: 0.5988893004553472\n",
            "Gradient Boosting Best Parameters: {'gb__learning_rate': 0.2, 'gb__max_depth': 3, 'gb__min_samples_leaf': 1, 'gb__min_samples_split': 2, 'gb__n_estimators': 100}\n",
            "Gradient Boosting Training RMSE: 1.6374933338122923\n",
            "Gradient Boosting Testing RMSE: 11.822831406202905\n",
            "Gradient Boosting Training R²: 0.9928356027771751\n",
            "Gradient Boosting Testing R²: 0.5884040421719319\n",
            "Random Forest Cross-Validation R² Scores: [  0.1557677  -10.75377857  -1.5015393   -1.80851438 -25.42892199]\n",
            "Gradient Boosting Cross-Validation R² Scores: [  0.12798238  -9.36325929  -2.56270583  -2.25837444 -21.11761531]\n",
            "Random Forest Average Cross-Validation R² Score: -7.86739730955909\n",
            "Gradient Boosting Average Cross-Validation R² Score: -7.034794497921872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "# Save the trained model\n",
        "joblib.dump(best_rf, '/content/random_forest_model.pkl')\n",
        "\n",
        "# Predicting on the test set\n",
        "y_pred = best_rf.predict(X_test)\n",
        "\n",
        "# Generate a CSV file with actual and predicted values\n",
        "results_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
        "results_df.to_csv('/content/actual_vs_predicted.csv', index=False)\n"
      ],
      "metadata": {
        "id": "cZU5ZM5wspvg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features_from_pdb(pdb_file):\n",
        "\n",
        "\n",
        "    features = []\n",
        "\n",
        "\n",
        "    print(\"Processing PDB file:\", pdb_file)\n",
        "\n",
        "\n",
        "    for i in range(27):\n",
        "        try:\n",
        "            feature = process_pdb(pdb_file, feature_name=f'feature{i+1}')\n",
        "            features.append(feature)\n",
        "            # Debugging: Print each feature after extraction\n",
        "            print(f\"Extracted feature{i+1}:\", feature)\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting feature{i+1}: {e}\")\n",
        "            features.append(np.nan)  # Handle missing features gracefully\n",
        "\n",
        "    return features\n",
        "\n",
        "# Define your `process_pdb` function\n",
        "def process_pdb(pdb_file, feature_name):\n",
        "\n",
        "    return np.random.rand()\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = joblib.load('random_forest_model.pkl')\n",
        "\n",
        "# Extract features from the new PDB file\n",
        "new_pdb_file = '/content/6ezq.pdb'\n",
        "new_features = extract_features_from_pdb(new_pdb_file)\n",
        "\n",
        "# Convert new_features to a numeric array and ensure it is a 2D array\n",
        "import numpy as np\n",
        "\n",
        "new_features = np.array(new_features, dtype=np.float32).reshape(1, -1)\n",
        "\n",
        "# Debugging: Print new_features to identify NaN values\n",
        "print(\"Extracted features:\", new_features)\n",
        "\n",
        "if np.isnan(new_features).any():\n",
        "    raise ValueError(\"The extracted features contain NaN values. Please check the feature extraction process.\")\n",
        "\n",
        "# Predict the melting temperature\n",
        "predicted_melting_temp = loaded_model.predict(new_features)\n",
        "print(f\"Predicted Melting Temperature: {predicted_melting_temp[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oz_1lNvXwsCI",
        "outputId": "6eb661b2-0820-4726-d93d-1eafaad30517"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing PDB file: /content/6ezq.pdb\n",
            "Extracted feature1: 0.5017757005426897\n",
            "Extracted feature2: 0.9466564318248974\n",
            "Extracted feature3: 0.6599164479149328\n",
            "Extracted feature4: 0.1499542552647165\n",
            "Extracted feature5: 0.445011850513794\n",
            "Extracted feature6: 0.9261430543612157\n",
            "Extracted feature7: 0.7230150083759516\n",
            "Extracted feature8: 0.35849041974595763\n",
            "Extracted feature9: 0.26578912717333514\n",
            "Extracted feature10: 0.9791491198141258\n",
            "Extracted feature11: 0.5485231923896424\n",
            "Extracted feature12: 0.40844712795107607\n",
            "Extracted feature13: 0.3355132396361262\n",
            "Extracted feature14: 0.9023689979534084\n",
            "Extracted feature15: 0.8687649680728191\n",
            "Extracted feature16: 0.8957148050772985\n",
            "Extracted feature17: 0.677242726625785\n",
            "Extracted feature18: 0.36578709216329297\n",
            "Extracted feature19: 0.4119984546525355\n",
            "Extracted feature20: 0.07518430004571963\n",
            "Extracted feature21: 0.9962371065996352\n",
            "Extracted feature22: 0.7730619646666697\n",
            "Extracted feature23: 0.7521138157609484\n",
            "Extracted feature24: 0.3770309449072885\n",
            "Extracted feature25: 0.8423735372997851\n",
            "Extracted feature26: 0.9083993157951901\n",
            "Extracted feature27: 0.5812171812118476\n",
            "Extracted features: [[0.5017757  0.9466564  0.65991646 0.14995426 0.44501185 0.92614305\n",
            "  0.723015   0.3584904  0.26578912 0.9791491  0.5485232  0.40844712\n",
            "  0.33551323 0.902369   0.868765   0.8957148  0.67724276 0.3657871\n",
            "  0.41199845 0.0751843  0.9962371  0.773062   0.7521138  0.37703094\n",
            "  0.84237355 0.90839934 0.58121717]]\n",
            "Predicted Melting Temperature: 72.06771091357605\n"
          ]
        }
      ]
    }
  ]
}